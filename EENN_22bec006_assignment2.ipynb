{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary libraries"
      ],
      "metadata": {
        "id": "I5pg5m2N5coR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, math, copy\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from torch.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
        "from torch.ao.quantization import get_default_qconfig_mapping"
      ],
      "metadata": {
        "id": "gEeNbxMD22iA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gloab Configuration"
      ],
      "metadata": {
        "id": "zLjuOp085ZF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CPU = torch.device(\"cpu\")\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "torch.backends.quantized.engine = \"fbgemm\"\n",
        "torch.manual_seed(7); np.random.seed(7)"
      ],
      "metadata": {
        "id": "WPqsyIvi5Sxq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Handeling"
      ],
      "metadata": {
        "id": "WjPOXgNi5nak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "3mBGjC9g5bxc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fully connected and convolution NN architecture"
      ],
      "metadata": {
        "id": "b1yXmO845yCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.drop1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.drop2 = nn.Dropout(0.2)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.drop1(self.relu1(self.fc1(x)))\n",
        "        x = self.drop2(self.relu2(self.fc2(x)))\n",
        "        lg = self.fc3(x)\n",
        "        return lg\n",
        "\n",
        "class FCNN_QAT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FCNN_QAT, self).__init__()\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        x = self.dequant(logits)\n",
        "        return x\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.drop3 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.drop3(self.relu3(self.fc1(x)))\n",
        "        lg = self.fc2(x)\n",
        "        return lg\n",
        "\n",
        "class CNN_QAT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_QAT, self).__init__()\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        x = self.dequant(logits)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tNdyX4JX5qel"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation functions"
      ],
      "metadata": {
        "id": "pxYXOFO66JD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, epochs=EPOCHS, fine_tune=False):\n",
        "    if fine_tune:\n",
        "        epochs = 3\n",
        "        print(f\"Fine-tuning {model.__class__.__name__}\")\n",
        "    else:\n",
        "        print(f\"Training {model.__class__.__name__}\")\n",
        "    model.to(DEVICE)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            loss = loss_func(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch [{epoch+1}/{epochs}]\\t | Loss: {loss.item():.4f}')\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "6HE8P6pk6A-3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "BOWF3sh66NNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _kmeans_quantize_tensor(t: torch.Tensor, n_clusters: int) -> torch.Tensor:\n",
        "    \"\"\"K-Means on a tensor with safe k selection.\"\"\"\n",
        "    arr = t.detach().cpu().view(-1, 1).numpy()\n",
        "    N = arr.shape[0]\n",
        "    if N < 2:\n",
        "        return t\n",
        "    uniq = np.unique(arr).size\n",
        "    k_eff = int(max(1, min(n_clusters, N, uniq)))\n",
        "    if k_eff == 1:\n",
        "        mean_val = float(arr.mean())\n",
        "        return torch.full_like(t, mean_val)\n",
        "    km = KMeans(n_clusters=k_eff, random_state=0, n_init=10)\n",
        "    labels = km.fit_predict(arr)\n",
        "    cents  = km.cluster_centers_.astype(np.float32)\n",
        "    q = torch.from_numpy(cents[labels].reshape(t.shape))\n",
        "    return q.to(t.dtype)\n",
        "\n",
        "def apply_kmeans_quantization(model, n_clusters=16):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            W = module.weight.data\n",
        "            module.weight.data = _kmeans_quantize_tensor(W, n_clusters).to(W.device)\n",
        "    return model\n",
        "\n",
        "def apply_linear_quantization(model, bits=4):\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            W = module.weight.data\n",
        "            v_max, v_min = W.max(), W.min()\n",
        "            if (v_max - v_min).abs() < 1e-12:\n",
        "                module.weight.data = torch.full_like(W, W.mean())\n",
        "                continue\n",
        "            amax = W.abs().max()\n",
        "            if amax < 1e-12:\n",
        "                module.weight.data = torch.zeros_like(W)\n",
        "                continue\n",
        "            scale = amax / (2**(bits-1) - 1)\n",
        "            q = torch.round(W / scale).clamp(-(2**(bits-1)), 2**(bits-1)-1)\n",
        "            Wdq = q * scale\n",
        "            module.weight.data = Wdq.to(W.device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "OjBMSQ_E6Mci"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions for comparision"
      ],
      "metadata": {
        "id": "z3I858wH6afJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_size_bytes_from_state_dict(sd, tmp_path):\n",
        "    torch.save(sd, tmp_path)\n",
        "    n = Path(tmp_path).stat().st_size\n",
        "    return n\n",
        "\n",
        "def param_count(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def latency_ms(model, device=torch.device(\"cpu\"), runs=50, warmup=10):\n",
        "    model = copy.deepcopy(model)\n",
        "    model.to(device).eval()\n",
        "    x = torch.randn(1,1,28,28, device=device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(warmup):\n",
        "            _ = model(x)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(runs):\n",
        "            _ = model(x)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    return (time.time()-t0)/runs * 1000.0\n",
        "\n",
        "def theoretical_kmeans_size_bits(model, n_clusters=16):\n",
        "    total_bits = 0\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            t = module.weight.detach().cpu().view(-1).numpy()\n",
        "            N = t.size\n",
        "            if N == 0:\n",
        "                continue\n",
        "            uniq = np.unique(t).size\n",
        "            k_eff = int(max(1, min(n_clusters, N, uniq)))\n",
        "            codebook_bits = k_eff * 32\n",
        "            index_bits = 0 if k_eff == 1 else math.ceil(math.log2(k_eff)) * N\n",
        "            total_bits += codebook_bits + index_bits\n",
        "    return total_bits\n",
        "\n",
        "def theoretical_linear_size_bits(model, bits=4):\n",
        "    total_bits = 0\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            total_bits += module.weight.numel() * bits\n",
        "    return total_bits"
      ],
      "metadata": {
        "id": "4A-LWF3H6ZqQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_qat(model_class, original_model_path, model_name):\n",
        "    \"\"\"\n",
        "    FX-QAT flow, also return quantized model + file size.\n",
        "    \"\"\"\n",
        "    qat_device = torch.device(\"cpu\")\n",
        "    model = model_class().to(qat_device)\n",
        "    model.load_state_dict(torch.load(original_model_path, map_location=qat_device))\n",
        "    model.eval()\n",
        "\n",
        "    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
        "    example_inputs = (next(iter(train_loader))[0],)  # a batch of images\n",
        "\n",
        "    prepared_model = prepare_qat_fx(model, qconfig_mapping, example_inputs)\n",
        "\n",
        "    print(f\"[QAT] Fine-tuning {model_name} on CPU...\")\n",
        "    global DEVICE\n",
        "    old_device = DEVICE\n",
        "    DEVICE = torch.device(\"cpu\")  # ensure train_model uses CPU\n",
        "    prepared_model.train()\n",
        "    train_model(prepared_model, train_loader, fine_tune=True)\n",
        "    DEVICE = old_device\n",
        "\n",
        "    prepared_model.eval()\n",
        "    quantized_model = convert_fx(prepared_model)\n",
        "\n",
        "    # Evaluate and measure size\n",
        "    old_device = DEVICE\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    accuracy = evaluate_model(quantized_model, test_loader)\n",
        "    DEVICE = old_device\n",
        "\n",
        "    size_bytes = model_size_bytes_from_state_dict(quantized_model.state_dict(), f\"{model_name}_qat_int8_sd.pth\")\n",
        "    return quantized_model, accuracy, size_bytes"
      ],
      "metadata": {
        "id": "mWG3bkVw6kpl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_original_models():\n",
        "    print(\"Training Original Models\")\n",
        "    fcnn_original = FCNN()\n",
        "    train_model(fcnn_original, train_loader)\n",
        "    torch.save(fcnn_original.state_dict(), \"FCNN_original.pth\")\n",
        "    original_fcnn_accuracy = evaluate_model(fcnn_original, test_loader)\n",
        "    print(f\"Original FCNN Test Accuracy: {original_fcnn_accuracy:.2f}%\\n\")\n",
        "\n",
        "    cnn_original = CNN()\n",
        "    train_model(cnn_original, train_loader)\n",
        "    torch.save(cnn_original.state_dict(), \"CNN_original.pth\")\n",
        "    original_cnn_accuracy = evaluate_model(cnn_original, test_loader)\n",
        "    print(f\"Original CNN Test Accuracy: {original_cnn_accuracy:.2f}%\\n\")\n",
        "    return original_fcnn_accuracy, original_cnn_accuracy"
      ],
      "metadata": {
        "id": "4sSnwupW6yVj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "4U2MD50y6-tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # 1) Train baselines\n",
        "    original_fcnn_accuracy, original_cnn_accuracy = train_original_models()\n",
        "\n",
        "    # Reload FP32 models on CPU for standardized measurements\n",
        "    fcnn_fp32 = FCNN().to(CPU)\n",
        "    fcnn_fp32.load_state_dict(torch.load(\"FCNN_original.pth\", map_location=CPU))\n",
        "    cnn_fp32 = CNN().to(CPU)\n",
        "    cnn_fp32.load_state_dict(torch.load(\"CNN_original.pth\", map_location=CPU))\n",
        "\n",
        "    # Params & measured state-dict sizes\n",
        "    fcnn_params = param_count(fcnn_fp32); cnn_params = param_count(cnn_fp32)\n",
        "    fcnn_fp32_size = model_size_bytes_from_state_dict(fcnn_fp32.state_dict(), \"FCNN_fp32_sd.pth\")\n",
        "    cnn_fp32_size  = model_size_bytes_from_state_dict(cnn_fp32.state_dict(),  \"CNN_fp32_sd.pth\")\n",
        "\n",
        "    fcnn_lat_cpu = latency_ms(fcnn_fp32, device=CPU)\n",
        "    cnn_lat_cpu  = latency_ms(cnn_fp32,  device=CPU)\n",
        "\n",
        "    # 2) PTQ: K-Means\n",
        "    print(\"\\nRunning K-Means Weight Quantization (Post-Training)\")\n",
        "    fcnn_kmeans = copy.deepcopy(fcnn_fp32)\n",
        "    fcnn_kmeans = apply_kmeans_quantization(fcnn_kmeans, n_clusters=16)\n",
        "    kmeans_fcnn_accuracy = evaluate_model(fcnn_kmeans, test_loader)\n",
        "\n",
        "    cnn_kmeans = copy.deepcopy(cnn_fp32)\n",
        "    cnn_kmeans = apply_kmeans_quantization(cnn_kmeans, n_clusters=16)\n",
        "    kmeans_cnn_accuracy = evaluate_model(cnn_kmeans, test_loader)\n",
        "\n",
        "    fcnn_km_size = model_size_bytes_from_state_dict(fcnn_kmeans.state_dict(), \"FCNN_kmeans_sd.pth\")\n",
        "    cnn_km_size  = model_size_bytes_from_state_dict(cnn_kmeans.state_dict(),  \"CNN_kmeans_sd.pth\")\n",
        "\n",
        "    fcnn_km_bits = theoretical_kmeans_size_bits(fcnn_fp32, n_clusters=16)\n",
        "    cnn_km_bits  = theoretical_kmeans_size_bits(cnn_fp32,  n_clusters=16)\n",
        "\n",
        "    # 3) PTQ: Linear (4-bit symmetric)\n",
        "    print(\"\\nRunning Linear Quantization (Post-Training)\")\n",
        "    fcnn_linear = copy.deepcopy(fcnn_fp32)\n",
        "    fcnn_linear = apply_linear_quantization(fcnn_linear, bits=4)\n",
        "    linear_fcnn_accuracy = evaluate_model(fcnn_linear, test_loader)\n",
        "\n",
        "    cnn_linear = copy.deepcopy(cnn_fp32)\n",
        "    cnn_linear = apply_linear_quantization(cnn_linear, bits=4)\n",
        "    linear_cnn_accuracy = evaluate_model(cnn_linear, test_loader)\n",
        "\n",
        "    fcnn_lq_size = model_size_bytes_from_state_dict(fcnn_linear.state_dict(), \"FCNN_linear4_sd.pth\")\n",
        "    cnn_lq_size  = model_size_bytes_from_state_dict(cnn_linear.state_dict(),  \"CNN_linear4_sd.pth\")\n",
        "\n",
        "    fcnn_lq_bits = theoretical_linear_size_bits(fcnn_fp32, bits=4)\n",
        "    cnn_lq_bits  = theoretical_linear_size_bits(cnn_fp32,  bits=4)\n",
        "\n",
        "    # 4) QAT (FX)\n",
        "    print(\"\\nRunning Quantization Aware Training (QAT)\")\n",
        "    fcnn_q_model, qat_fcnn_accuracy, fcnn_q_size = run_qat(FCNN_QAT, \"FCNN_original.pth\", \"FCNN\")\n",
        "    cnn_q_model,  qat_cnn_accuracy,  cnn_q_size  = run_qat(CNN_QAT,  \"CNN_original.pth\",  \"CNN\")\n",
        "\n",
        "    # 5) Results table\n",
        "    rows = []\n",
        "    def add_row(name, method, model_obj, acc, size_bytes, theo_bits=None, lat_cpu=None, params=None):\n",
        "        rows.append(dict(\n",
        "            model=name,\n",
        "            method=method,\n",
        "            acc_pct=round(float(acc), 2),\n",
        "            params=int(params if params is not None else param_count(model_obj)),\n",
        "            file_mb_measured=round(size_bytes/(1024**2), 3) if size_bytes is not None else None,\n",
        "            theoretical_mb=round((theo_bits/8)/(1024**2), 3) if theo_bits is not None else None,\n",
        "            latency_ms_cpu=round(lat_cpu, 3) if lat_cpu is not None else None\n",
        "        ))\n",
        "\n",
        "    add_row(\"FCNN\", \"FP32 (baseline)\", fcnn_fp32, original_fcnn_accuracy, fcnn_fp32_size,\n",
        "            theo_bits=None, lat_cpu=fcnn_lat_cpu, params=fcnn_params)\n",
        "    add_row(\"CNN\",  \"FP32 (baseline)\", cnn_fp32,  original_cnn_accuracy,  cnn_fp32_size,\n",
        "            theo_bits=None, lat_cpu=cnn_lat_cpu,  params=cnn_params)\n",
        "\n",
        "    add_row(\"FCNN\", \"PTQ KMeans(K=16)\", fcnn_kmeans, kmeans_fcnn_accuracy, fcnn_km_size,\n",
        "            theo_bits=fcnn_km_bits, lat_cpu=latency_ms(fcnn_kmeans, CPU), params=fcnn_params)\n",
        "    add_row(\"CNN\",  \"PTQ KMeans(K=16)\", cnn_kmeans,  kmeans_cnn_accuracy,  cnn_km_size,\n",
        "            theo_bits=cnn_km_bits,  lat_cpu=latency_ms(cnn_kmeans,  CPU), params=cnn_params)\n",
        "\n",
        "    add_row(\"FCNN\", \"PTQ Linear(4-bit sym)\", fcnn_linear, linear_fcnn_accuracy, fcnn_lq_size,\n",
        "            theo_bits=fcnn_lq_bits, lat_cpu=latency_ms(fcnn_linear, CPU), params=fcnn_params)\n",
        "    add_row(\"CNN\",  \"PTQ Linear(4-bit sym)\", cnn_linear,  linear_cnn_accuracy,  cnn_lq_size,\n",
        "            theo_bits=cnn_lq_bits,  lat_cpu=latency_ms(cnn_linear,  CPU), params=cnn_params)\n",
        "\n",
        "    add_row(\"FCNN\", \"QAT INT8 (FX)\", fcnn_q_model, qat_fcnn_accuracy, fcnn_q_size,\n",
        "            theo_bits=None, lat_cpu=latency_ms(fcnn_q_model, CPU), params=fcnn_params)\n",
        "    add_row(\"CNN\",  \"QAT INT8 (FX)\", cnn_q_model,  qat_cnn_accuracy,  cnn_q_size,\n",
        "            theo_bits=None, lat_cpu=latency_ms(cnn_q_model,  CPU), params=cnn_params)\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\n",
        "        \"model\",\"method\",\"acc_pct\",\"params\",\"file_mb_measured\",\"theoretical_mb\",\"latency_ms_cpu\"\n",
        "    ])\n",
        "    print(\"\\n=== RESULTS (before/after) ===\")\n",
        "    print(df.to_string(index=False))\n",
        "    df.to_csv(\"quant_results.csv\", index=False)\n",
        "    print(\"\\nSaved: quant_results.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXKViU2963-H",
        "outputId": "044f3333-d487-4d6d-d7f7-cec8d69da0f9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Original Models\n",
            "Training FCNN\n",
            "Epoch [1/5]\t | Loss: 0.0143\n",
            "Epoch [2/5]\t | Loss: 0.0099\n",
            "Epoch [3/5]\t | Loss: 0.0056\n",
            "Epoch [4/5]\t | Loss: 0.2504\n",
            "Epoch [5/5]\t | Loss: 0.0085\n",
            "Original FCNN Test Accuracy: 98.10%\n",
            "\n",
            "Training CNN\n",
            "Epoch [1/5]\t | Loss: 0.0089\n",
            "Epoch [2/5]\t | Loss: 0.0040\n",
            "Epoch [3/5]\t | Loss: 0.0880\n",
            "Epoch [4/5]\t | Loss: 0.0043\n",
            "Epoch [5/5]\t | Loss: 0.0008\n",
            "Original CNN Test Accuracy: 99.10%\n",
            "\n",
            "\n",
            "Running K-Means Weight Quantization (Post-Training)\n",
            "\n",
            "Running Linear Quantization (Post-Training)\n",
            "\n",
            "Running Quantization Aware Training (QAT)\n",
            "[QAT] Fine-tuning FCNN on CPU...\n",
            "Fine-tuning GraphModule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-941097144.py:13: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  prepared_model = prepare_qat_fx(model, qconfig_mapping, example_inputs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3]\t | Loss: 0.0383\n",
            "Epoch [2/3]\t | Loss: 0.0244\n",
            "Epoch [3/3]\t | Loss: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-941097144.py:24: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = convert_fx(prepared_model)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[QAT] Fine-tuning CNN on CPU...\n",
            "Fine-tuning GraphModule\n",
            "Epoch [1/3]\t | Loss: 0.0129\n",
            "Epoch [2/3]\t | Loss: 0.0003\n",
            "Epoch [3/3]\t | Loss: 0.0443\n",
            "\n",
            "=== RESULTS (before/after) ===\n",
            "model                method  acc_pct  params  file_mb_measured  theoretical_mb  latency_ms_cpu\n",
            " FCNN       FP32 (baseline)    98.10  535818             2.047             NaN           0.181\n",
            "  CNN       FP32 (baseline)    99.10  421642             1.612             NaN           0.678\n",
            " FCNN      PTQ KMeans(K=16)    98.06  535818             2.047           0.255           0.194\n",
            "  CNN      PTQ KMeans(K=16)    99.09  421642             1.612           0.201           0.713\n",
            " FCNN PTQ Linear(4-bit sym)    98.13  535818             2.047           0.255           0.186\n",
            "  CNN PTQ Linear(4-bit sym)    98.99  421642             1.612           0.201           0.694\n",
            " FCNN         QAT INT8 (FX)    97.75  535818             0.532             NaN           0.234\n",
            "  CNN         QAT INT8 (FX)    99.07  421642             0.415             NaN           0.576\n",
            "\n",
            "Saved: quant_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZy4jAAd7BSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}